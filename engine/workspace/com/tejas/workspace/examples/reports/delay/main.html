<html>
<head><title>Delay metrics</title></head>
<body>

<h1>Contents</h1>

<ul>
	<li><a href='#sectionIntroduction'>Introduction</a></li>
	<li><a href='#sectionLinkAndNodeModel'>Link and node model</a></li>
	<li><a href='#sectionTrafficAndQueuingModel'>Traffic and queuing delay model</a></li>
	<li><a href='#sectionE2EPathDelayModel'>End-to-end path delay model</a></li>
	<li><a href='#sectionAverageNetworkDelay'>Average network delay</a></li>
	<li><a href='#sectionInformationTables'>Information tables</a>
	    <ul>
		    <li><a href='#linkDelayTable'>Link delay information</a></li>
		    <li><a href='#pathDelayTable'>Path delay information</a></li>
		    <li><a href='#networkDelayTable'>Network-wide delay information</a></li>
	    </ul>
	</li>
	<li><a href='#sectionReferences'>References</a></li>
</ul>

<a name='sectionIntroduction'><h1>Introduction</h1></a>

<p>In packet-switched networks, traffic sources split data into smaller pieces called packets, and transmit them attached to a header with control information. Per each packet received, packet-switching nodes read its header and take appropriate forwarding decisions, according to the routing plan configured. In real networks, traffic is highly unpredictable and thus modelled as random processes. When we say that a traffic source <i>d</i> generates <i>h<sub>d</sub></i> traffic units, we refer to a time average. Instantaneous traffic generated oscillates very fast and uncontrollably from peak intervals (generating more traffic than the average) to valley intervals (generating less traffic than the average).</p>

<p>The traffic carried by a link is the aggregation (multiplexing) of the traffic from all the sources routed through this link. At a given moment, it is very likely that some of the sources aggregated are in a peak, while others are in a valley. They compensate each other. Thus, you do not need a link capacity equal to the <i>sum of the peak</i> traffics of all the sources, but a capacity between the <i>sum of the averages</i> and the sum of the peaks. We say that multiplexing sources provides an <i>statistical multiplexing gain</i>.</p>

<p>Statistical multiplexing gain is the powering fact propeling packet switching. Since it is very common that sources have peaks of traffic several times higher than their average (e.g. 2 or 3 times). However, at unpredictable moments in time, peak traffic intervals coincide and link capacities are not enough to forward traffic. Then, nodes store packets in queues, so they are delayed until they can be transmitted (this delay is known as queuing delay). If this situation remains, queues are filled provoking packet drops. We say that the link is congested or saturated. Note that if the link capacity is below the <i>sum of the average</i> traffic generated by the sources traversing the link, a large amount of packet drops will always occur, whatever buffer you allocate in the nodes. Network designs must enforce always that link capacities are not below the sum of the averages of the traffics to be carried.</p>

<p>Network design tries to model statistically delays and drops in order to minimize their effects. Traffic models capture not only the average of each traffic source, but also a measure of its <i>burstiness</i>. Intuitively, it is clear that the more steep and long that the peak-valley intervals are (or the more <i>bursty</i> the traffic is), the higher the queuing delay. This is because, during low load intervals, the link is underutilized with negligible delays, but during peak traffic intervals packets need to be buffered and can suffer large queuing delays or drops. Naturally, a zero queuing delay occurs when the traffic is perfectly constant (not random).</p>

<p>In the following sections we describe the link, node and traffic model applied in this report to estimate average packet delay measured.</p>

<a name='sectionLinkAndNodeModel'><h1>Link and node model</h1></a>

<p>Each node has a first-in-first-out queue for each output links, where packets are stored prior to transmission. We assume that buffers are infinite, and thus no packet losses occur (unless the average traffic routed through a link is higher than its capacity, in which case the queuing delay would grow to infinite in our model).</p>

<p>The delay of the packets traversing a link is composed of three parts:</p>
<ul>
	<li>Queuing delay or buffering delay (<i>T<sub>e</sub><sup>b</sup></i>): Time spent in the queue.
	<li>Transmission delay (<i>T<sub>e</sub><sup>tx</sup></i>): Time spent in transmitting the packet. This is given by the ratio between the packet length in bits, and the link binary rate in bps. The binary rate of a link in <i>net2plan</i> is obtained multiplying the link capacity <i>u<sub>e</sub></i> (measured in Erlangs in net2plan), by the <i>binaryRateInBitsPerSecondPerErlang</i> value, which can be configured in the <i>File-Options</i> menu.</li>
	<li>Propagation delay (<i>T<sub>e</sub><sup>prop</sup></i>): Time in propagating the packet signal along the link. Equal to the ratio between the link length (i.e. km) and the propagation speed (i.e. km/sec). The propagation speed value for the links can also be set in the <i>File-Options</i> menu.</li>
</ul>

<a name='sectionTrafficAndQueuingModel'><h1>Traffic and queuing delay model</h1></a>

<p>We assume that each demand <i>d</i> is an inelastic source of traffic of average load <i>h<sub>d</sub></i>. Inelastic source means that the amount of traffic injected to the network, does not depend on the network state (e.g. the source does not inject more traffic if the network capacity is doubled, nor less traffic if the network capacity is halved). <i>Net2Plan</i> assumes that the value <i>h<sub>d</sub></i> coming from the traffic matrices is measured in Erlangs. The traffic average in bps is obtained multiplying this quantity by the <i>binaryRateInBitsPerSecondPerErlang</i> value (configured in the <i>File-Options</i> menu). Average packet size in bits (which we denote as <i>L</i> in this report), is directly taken from the <i>averagePacketLengthInBytes</i> value configurable in the <i>File-Options</i> menu.</p>

<p>The traffic in each link is the aggregation of the traffic from the demands routed through each link. We assume that the packet arrivals in each link are independent from each other, and follow a self-similar pattern with Hurst parameter <i>H &isin; [0.5, 1)</i>. Roughly speaking, self-similarity in the traffic means that the traffic is bursty at different time scales. That is, if we observe the accummulated traffic each millisecond, we see oscillations between "peak" milliseconds and "valley" milliseconds, which follow a similar statistical pattern as the oscillations we would observe if we saw the traffic in accummulations of 10 milliseconds, or 100 milliseconds (that is the reason of using the word "self-similar"). In contrast, in classical Poisson trafic models, when we observe the traffic in higher accumulation intervals, the traffic becomes more constant (more predictable).</p>

<p>Self-similar distributions are characterized by the Hurst parameter <i>H &isin; [0.5, 1)</i>. The higher <i>H</i> (<i>H&asymp;1</i>), the more self-similar the traffic is. A Hurst parameter <i>H=0.5</i> characterizes non self-similar traffic (i.e. Poisson traffic has a parameter <i>H=0.5</i>). Some measurements in traffic volumes in the Internet links has shown self-similar distributions of Hurst parameters between <i>H=0.6</i> and <i>H=0.9</i>.</p>

<p>There are many models to estimate queuing delays for queues fed with self-similar traffic. Estimations are usually very complex, out of the interest of this report. What we use in this report is the simple estimation in [<a href='#Stallings2002'>1</a>] for the average queuing delay <i>T<sub>e</sub><sup>b</sup></i> given by:</p>

<center>
	<img src='t_e_buf.png' />
</center>

<!--
T_e^b = (L/u_e)\frac{\rho_e ^ {1/2(1-H)}}{(1-\rho_e)^{H/(1-H)}}
-->

<p>Where <i>&rho;<sub>e</sub>=y<sub>e</sub>/u<sub>e</sub></i> is the average utilization of the link. When <i>H=0.5</i>, the previous formula matches the average queuing delay for a M/M/1 queue.</p>

<a name='sectionE2EPathDelayModel'><h1>End-to-end path delay model</h1></a>

<p>The average end-to-end delay <i>T<sub>p</sub></i> for a path <i>p</i> is the sum of the average link delays for the traversed links of the path:</p>

<center>
	<img src='t_p.png' />
</center>

<!--
T_p = \sum_{e \in p} T_e = \sum_{e \in p} T_e^b + T_e^{tx} + T_e^{prop}
-->

<a name='sectionAverageNetworkDelay'><h1>Average network delay</h1></a>

<p>It is possible to obtain an average network delay measure, defined as the average end-to-end delay suffered by a packet chosen randomly in the network. The average network delay <i>T</i> is given by:</p>

<center>
	<img src='t.png' />
</center>

<!--
T = \frac{1}{\sum_d h_d} \sum_{e \in E} y_e T_e
-->

<p>where <i>y<sub>e</sub></i> is the total amount of traffic carried in a link <i>e</i>. Note that we are assuming that all the traffic <i>h<sub>d</sub></i> of each demand is carried, and thus the avarage traffic in each link is below the link capacity.

In our report we provide two measures for <i>T</i>: one considering queuing, transmission and propagation delays in the links, and one considering only propagation delays. In general, the higher the bit rate, the smaller that queuing and transmission delays become. In contrast, the larger (in km) that the links are, the higher the propagation delays become. In WAN core networks, characterized by high bit rates and large distances, it is common to neglect queuing and transmission delays in the calculations. Providing the two values for <i>T</i> helps us to estimate if neglecting all the delays but propagation is a valid approximation.

<a name='sectionInformationTables'></a>
<h1>Information tables</h1>

<p><a name='linkDelayTable'>Link delay information</a></p>

#linkDelayTable#

<p><a name='pathDelayTable'>Path delay information</a></p>

#pathDelayTable#

<p><a name='networkDelayTable'>Network-wide delay information</a></p>

#networkDelayTable#

<a name='sectionReferences'></a>
<h1>References</h1>

<a name='Stallings2002'></a>
[1] W. Stallings, <i>High-Speed Networks and Internets: Performance and Quality of Service</i>, Prentice Hall, 2002.
</body>
</html>